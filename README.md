# llm_optimizations
All the methods to optimize LLM Inference and training.
1. Paged Attention for KV cache memory
