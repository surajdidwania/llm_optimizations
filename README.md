# LLM Optimizations in training and Inference
All the methods to optimize LLM Inference and training.
1. Paged Attention for KV cache memory
